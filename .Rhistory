import <- as.data.frame(import)
row.names(import) <- names(fit$model[2:nvar])
names(import) <- "Weights"
import <- import[order(import),1, drop=FALSE]
dotchart(import$Weights, labels=row.names(import),
xlab="% of R-Square", pch=19,
main="Relative Importance of Predictor Variables",
sub=paste("Total R-Square=", round(rsquare, digits=3)),
...)
return(import)
}
relweights(best.fit, col="black")
#의사 결정 나무
library(rpart)
set.seed(2019)
index = sample(nrow(data), size = nrow(data) *0.7 , replace = F)
data
train = data[index,]#train : 1353개
train
str(train)
test = data[-index,] #test : 581개
test
str(test)
test_x = as.data.frame(test[,c(2:22)])
test_y = test[,1]
my.control<-rpart.control(xval=10,cp=0,minsplit=nrow(train)*0.05)
tree.fit<-rpart(k_pm10~.,data=train,method='anova',control=my.control)
printcp(tree.fit)
which.min(tree.fit$cptable[,'xerror'])
#가지 치기
prune.tree.fit = prune(tree.fit,cp=tree.fit$cptable[31])
prune.tree.fit
summary(prune.tree.fit)
#피처 중요도 확인
prune.tree.fit$variable.importance
#트리 시각화
plot(prune.tree.fit,uniform=T,margin=0.1)
text(prune.tree.fit,use.n=T,col='blue',cex=0.8)
png("decisionTree.png",width = 2000, height = 1500)
dev.off()
Actual = test_y
Forecast<-predict(prune.tree.fit,newdata=test_x,type='vector')
Forecast = round(Forecast)
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
#모델 검증
mse(tmp$Actual, tmp$Forecast) #74.65232
rmse(tmp$Actual, tmp$Forecast) #7.862524
mape(tmp$Actual, tmp$Forecast) #8.637269
mase(tmp$Actual, tmp$Forecast,1) #0.3554614
cor(tmp$Actual, tmp$Forecast) # 0.8974767
#랜덤 포레스트
library(randomForest)
set.seed(2019)
index = sample(nrow(data), size = nrow(data) *0.7 , replace = F)
set.seed(2019)
index = sample(nrow(data), size = nrow(data) *0.7 , replace = F)
data
train = data[index,]#train : 1353개
train
str(train)
test = data[-index,] #test : 581개
test
str(test)
test_x = as.data.frame(test[,c(2:22)])
test_y = test[,1]
rf.fit<-randomForest(k_pm10~.,data=train,ntree=100,mtry=5,importance=T,na.action=na.omit)
rf.fit
importance(rf.fit,type=1)
Actual = test_y
Forecast = predict(rf.fit,newdata=test_x)
Forecast = round(Forecast)
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
#모델 검증
mse(tmp$Actual, tmp$Forecast) #61.48365
rmse(tmp$Actual, tmp$Forecast) #7.841151
mape(tmp$Actual, tmp$Forecast) # 0.1282503
mase(tmp$Actual, tmp$Forecast,1) #0.3128823
cor(tmp$Actual, tmp$Forecast) #0.9177493
#시각화
ggplot = ggplot(tmp, aes(x=Forecast, y=Actual)) + geom_point() + geom_smooth(method = lm) +labs(title = "Forecast versus Actuals - RandomForest")
ggplot
ggsave("randomForest.png",plot = ggplot,width = 10, height = 5)
#xgboost
library(xgboost)
set.seed(2019)
index = sample(nrow(data), size = nrow(data) *0.7 , replace = F)
data
train = data[index,]#train : 1353개
train
str(train)
test = data[-index,] #test : 581개
test
str(test)
train_x = as.matrix(train[,c(2:22)])
train_y = train[,1]
test_x = as.matrix(test[,c(2:22)])
test_y = test[,1]
xgb.train = xgb.DMatrix(data = train_x, label = train_y)
xgb.test =  xgb.DMatrix(data = test_x, label = test_y)
params = list(booster="gbtree",eta=0.001,max_depth=5,gamma=3,subsample=0.75,colsample_bytree=1,objective="reg:linear",eval_metric="rmse")
xgb.fit=xgb.train(params=params,data=xgb.train,nrounds=10000,nthreads=1,early_stopping_rounds=10,watchlist=list(val1=xgb.train,val2=xgb.test),verbose=0)
xgb.fit
Forecast = predict(xgb.fit, test_x,reshape=T)
Forecast  = round(Forecast)
Actual = test_y
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
#모델 검증
mse(tmp$Actual, tmp$Forecast) #52.18933
rmse(tmp$Actual, tmp$Forecast) #7.224218
mape(tmp$Actual, tmp$Forecast) #0.09937504
mase(tmp$Actual, tmp$Forecast,1) #0.2671256
cor(tmp$Actual, tmp$Forecast) #0.9287739
ggplot=ggplot(tmp, aes(x=Forecast, y=Actual)) + geom_point() + geom_smooth(method = lm) +labs(title = "Forecast versus Actuals - xgboost")
ggplot
ggsave("xgboost.png",plot = ggplot,width = 10, height = 5)
importance_matrix <- xgb.importance(model = xgb.fit)
importance_matrix
xgb.plot.importance(importance_matrix = importance_matrix)
ggsave("xgboost_importance.png",plot = ggplot,width = 10, height = 5)
ggplot = xgb.plot.importance(importance_matrix = importance_matrix)
ggsave("xgboost_importance.png",plot = ggplot,width = 10, height = 5)
png("importance_matrix.png",width = 2000, height = 1500)
ggplot = xgb.plot.importance(importance_matrix = importance_matrix)
dev.off()
getwd()
beijing_dust= read.csv("final_beijing_dust.csv")
head(beijing_dust)
names(beijing_dust) = c("date","ch_pm2.5")
head(beijing_dust,10)
beijing_climate= read.csv("beijing_climate.csv")
head(beijing_climate)
names(beijing_climate) = c("date","ch_avgtemp","ch_humidity","ch_rain","ch_sight","ch_wind")
head(beijing_climate,10)
seoul_dust= read.csv("seoul_dust.csv")
head(seoul_dust)
names(seoul_dust) = c("date","k_pm10","k_pm2.5","k_O3","k_NO2","k_CO","k_SO2")
head(seoul_dust,10)
seoul_climate= read.csv("seoul_climate.csv")
head(seoul_climate)
names(seoul_climate) = c("date","k_avgtemp","k_rain","k_wind","k_wind_direct","k_humidity")
head(seoul_climate,10)
library(dplyr)
beijing = left_join(beijing_dust,beijing_climate, by = "date")
beijing
seoul = left_join(seoul_dust,seoul_climate, by = "date")
seoul
seoul$date=gsub("-", "", seoul$date)
seoul$date = as.numeric(seoul$date)
data = left_join(seoul, beijing, by = "date")
head(data,10)
summary(data)
#결측치 제거
library(DataExplorer)
plot_missing(data)
data$ch_rain = ifelse(is.na(data$ch_rain), 0, data$ch_rain)
data$k_rain = ifelse(is.na(data$k_rain), 0, data$k_rain)
data$ch_sight <- ifelse(is.na(data$ch_sight), round(mean(data$ch_sight,na.rm = T),2), data$ch_sight)
table(data$k_wind_direct)
data$k_wind_direct <- ifelse(is.na(data$k_wind_direct), 270, data$k_wind_direct)
data$k_wind <- ifelse(is.na(data$k_wind), round(mean(data$k_wind,na.rm = T),1), data$k_wind)
data$ch_wind <- ifelse(is.na(data$ch_wind), round(mean(data$ch_wind,na.rm = T),1), data$ch_wind)
data$ch_avgtemp <- ifelse(is.na(data$ch_avgtemp), round(mean(data$ch_avgtemp,na.rm = T),1), data$ch_avgtemp)
data$ch_humidity <- ifelse(is.na(data$ch_humidity), round(mean(data$ch_humidity,na.rm = T)), data$ch_humidity)
data$ch_pm2.5 <- ifelse(is.na(data$ch_pm2.5), round(mean(data$ch_pm2.5,na.rm = T),5), data$ch_pm2.5)
plot_missing(data)
summary(data)
#이상치 제거
data$k_pm10 = ifelse(data$k_pm10>=93, 93, data$k_pm10)
data$k_pm2.5 = ifelse(data$k_pm2.5>=52, 52, data$k_pm2.5)
data$k_O3 = ifelse(data$k_O3>=0.060, 0.060, data$k_O3)
data$k_NO2 = ifelse(data$k_NO2>=0.063, 0.063, data$k_NO2)
data$k_CO = ifelse(data$k_CO>=0.8, 0.8, data$k_CO)
data$k_SO2 = ifelse(data$k_SO2>=0.006, 0.006, data$k_SO2)
data$k_SO2 = ifelse(data$k_SO2<=0.003, 0.003, data$k_SO2)
data$k_wind = ifelse(data$k_wind>=3.9, 3.9, data$k_wind)
data$ch_pm2.5 = ifelse(data$ch_pm2.5>=128.48042, 128.48042, data$ch_pm2.5)
data$ch_wind = ifelse(data$ch_wind>=19.1, 19.1, data$ch_wind)
summary(data)
#이상치 제거
boxplot(data$k_pm10)$stat
#이상치 제거
data$k_pm10 = ifelse(data$k_pm10<=28, 28, data$k_pm10)
summary(data)
#이상치 제거
boxplot(data$k_humidity)
quantile(data$k_pm10, 3/4)
getwd()
beijing_dust= read.csv("final_beijing_dust.csv")
head(beijing_dust)
names(beijing_dust) = c("date","ch_pm2.5")
head(beijing_dust,10)
beijing_climate= read.csv("beijing_climate.csv")
head(beijing_climate)
names(beijing_climate) = c("date","ch_avgtemp","ch_humidity","ch_rain","ch_sight","ch_wind")
head(beijing_climate,10)
seoul_dust= read.csv("seoul_dust.csv")
head(seoul_dust)
names(seoul_dust) = c("date","k_pm10","k_pm2.5","k_O3","k_NO2","k_CO","k_SO2")
head(seoul_dust,10)
seoul_climate= read.csv("seoul_climate.csv")
head(seoul_climate)
names(seoul_climate) = c("date","k_avgtemp","k_rain","k_wind","k_wind_direct","k_humidity")
head(seoul_climate,10)
library(dplyr)
beijing = left_join(beijing_dust,beijing_climate, by = "date")
beijing
seoul = left_join(seoul_dust,seoul_climate, by = "date")
seoul
seoul$date=gsub("-", "", seoul$date)
seoul$date = as.numeric(seoul$date)
data = left_join(seoul, beijing, by = "date")
head(data,10)
summary(data)
#결측치 제거
library(DataExplorer)
plot_missing(data)
data$ch_rain = ifelse(is.na(data$ch_rain), 0, data$ch_rain)
data$k_rain = ifelse(is.na(data$k_rain), 0, data$k_rain)
data$ch_sight <- ifelse(is.na(data$ch_sight), round(mean(data$ch_sight,na.rm = T),2), data$ch_sight)
table(data$k_wind_direct)
data$k_wind_direct <- ifelse(is.na(data$k_wind_direct), 270, data$k_wind_direct)
data$k_wind <- ifelse(is.na(data$k_wind), round(mean(data$k_wind,na.rm = T),1), data$k_wind)
data$ch_wind <- ifelse(is.na(data$ch_wind), round(mean(data$ch_wind,na.rm = T),1), data$ch_wind)
data$ch_avgtemp <- ifelse(is.na(data$ch_avgtemp), round(mean(data$ch_avgtemp,na.rm = T),1), data$ch_avgtemp)
data$ch_humidity <- ifelse(is.na(data$ch_humidity), round(mean(data$ch_humidity,na.rm = T)), data$ch_humidity)
data$ch_pm2.5 <- ifelse(is.na(data$ch_pm2.5), round(mean(data$ch_pm2.5,na.rm = T),5), data$ch_pm2.5)
plot_missing(data)
summary(data)
#이상치 제거
data$k_pm10 = ifelse(data$k_pm10>=93, 93, data$k_pm10)
data$k_pm2.5 = ifelse(data$k_pm2.5>=52, 52, data$k_pm2.5)
data$k_O3 = ifelse(data$k_O3>=0.060, 0.060, data$k_O3)
data$k_NO2 = ifelse(data$k_NO2>=0.063, 0.063, data$k_NO2)
data$k_CO = ifelse(data$k_CO>=0.8, 0.8, data$k_CO)
data$k_SO2 = ifelse(data$k_SO2>=0.006, 0.006, data$k_SO2)
data$k_SO2 = ifelse(data$k_SO2<=0.003, 0.003, data$k_SO2)
data$k_wind = ifelse(data$k_wind>=3.9, 3.9, data$k_wind)
data$ch_pm2.5 = ifelse(data$ch_pm2.5>=128.48042, 128.48042, data$ch_pm2.5)
data$ch_wind = ifelse(data$ch_wind>=19.1, 19.1, data$ch_wind)
summary(data)
str(data)
getwd()
beijing_dust= read.csv("final_beijing_dust.csv")
head(beijing_dust)
names(beijing_dust) = c("date","ch_pm2.5")
head(beijing_dust,10)
beijing_climate= read.csv("beijing_climate.csv")
head(beijing_climate)
names(beijing_climate) = c("date","ch_avgtemp","ch_humidity","ch_rain","ch_sight","ch_wind")
head(beijing_climate,10)
seoul_dust= read.csv("seoul_dust.csv")
head(seoul_dust)
names(seoul_dust) = c("date","k_pm10","k_pm2.5","k_O3","k_NO2","k_CO","k_SO2")
head(seoul_dust,10)
seoul_climate= read.csv("seoul_climate.csv")
head(seoul_climate)
names(seoul_climate) = c("date","k_avgtemp","k_rain","k_wind","k_wind_direct","k_humidity")
head(seoul_climate,10)
library(dplyr)
beijing = left_join(beijing_dust,beijing_climate, by = "date")
beijing
seoul = left_join(seoul_dust,seoul_climate, by = "date")
seoul
seoul$date=gsub("-", "", seoul$date)
seoul$date = as.numeric(seoul$date)
data = left_join(seoul, beijing, by = "date")
head(data,10)
summary(data)
#결측치 제거
library(DataExplorer)
plot_missing(data)
data$ch_rain = ifelse(is.na(data$ch_rain), 0, data$ch_rain)
data$k_rain = ifelse(is.na(data$k_rain), 0, data$k_rain)
data$ch_sight <- ifelse(is.na(data$ch_sight), round(mean(data$ch_sight,na.rm = T),2), data$ch_sight)
table(data$k_wind_direct)
data$k_wind_direct <- ifelse(is.na(data$k_wind_direct), 270, data$k_wind_direct)
data$k_wind <- ifelse(is.na(data$k_wind), round(mean(data$k_wind,na.rm = T),1), data$k_wind)
data$ch_wind <- ifelse(is.na(data$ch_wind), round(mean(data$ch_wind,na.rm = T),1), data$ch_wind)
data$ch_avgtemp <- ifelse(is.na(data$ch_avgtemp), round(mean(data$ch_avgtemp,na.rm = T),1), data$ch_avgtemp)
data$ch_humidity <- ifelse(is.na(data$ch_humidity), round(mean(data$ch_humidity,na.rm = T)), data$ch_humidity)
data$ch_pm2.5 <- ifelse(is.na(data$ch_pm2.5), round(mean(data$ch_pm2.5,na.rm = T),5), data$ch_pm2.5)
plot_missing(data)
summary(data)
#이상치 제거
boxplot(data$k_pm10)
#이상치 제거
boxplot(data$k_pm10)$stat
quantile(data$k_pm10, 2/4)
quantile(data$k_pm10, 3/4)
quantile(data$k_pm10, 4/4)
quantile(data$k_pm10, 1/4)
library(arules)
library(arulesViz)
install.packages("arulesViz")
library(arulesViz)
library(arulesViz)
data("Groceries")
head(Groceries)
str(Groceries)
itemFrequencyPlot(Groceries, topN = 10, type="absolute")
itemFrequencyPlot(Groceries, topN = 15)
#모델
rules = apriori(Groceries, parameter = list(supp=0.001, conf=0.9,maxlen=4))
rules
options(digits = 2)
rules = sort(rules, by="lift", decreasing = T)
inspect(rules[1:5])
rules = sort(rules, by="confidence", decreasing = T)
inspect(rules[1:5])
tab = crossTable(Groceries)
tab[1:3, 1:3]
table["bottled beer","bottled beer"]
str(Groceries)
itemFrequencyPlot(Groceries, topN = 10, type="absolute")
table["bottled beer","canned beer"]
tab["bottled beer","bottled beer"]
tab["bottled beer","canned beer"]
beer.rules = apriori(Groceries,  parameter = list(support = 0.0015, confidence = 0.3),
apperance = list(default = "lhs", rhs="bottled bear"))
beer.rules = apriori(Groceries,  parameter = list(supp = 0.0015, conf = 0.3),
apperance = list(default = "lhs", rhs="bottled bear"))
beer.rules = apriori(Groceries,  parameter = list(supp = 0.0015, conf = 0.3),
appearance = list(default = "lhs", rhs="bottled bear"))
beer.rules = apriori(Groceries,  parameter = list(supp = 0.0015, conf = 0.3),
appearance = list(default = "lhs", rhs="bottled beer"))
beer.rules = sort(beer.rules, decreasing =T, by="lift")
inspect(beer.rules)
tab["bottled beer","red/blush wine"]
tab["red/blush wine","red/blush wine"]
48/189
tab["white wine","white wine"]
tab["bottled beer","white wine"]
22/187
plot(beer.rules, method="graph",measure = "lift",shading = "confidence")
View(beijing)
#추천 엔진 - 사용자 기반 협업 필터링, 아이템 기반 협업 필터링
ratings = c(3,5,5,5,1,1,5,2,5,1,1,5,3,5,1,5,4,2,4,3,4,2,1,4)
ratingMat = matrix(ratings, nrow=6)
rownames(ratingMat) = c("Homor","Marge","Bart","Lisa","Flanders","Me")
colnames(ratingMat) = c("Avengers","American Sniper","Les Miserable","Mad max")
ratingMat
svd = svd(ratingMat)
svd
sum(svd$d)
var = sum(svd$d[1:2])
var
var/sum(svd$d)
n = 4
f1(svd)
f1 = function(x){
score = 0
for(i in 1:n){
score = score + svd$u[,i] %*% t(svd$v[,i]*svd$d[i])
return(score)
}
}
n = 4
f1(svd)
n=2
f1(svd)
library(psych)
pca = principal(ratingMat, nfactors = 2, rotate = "none")
pca
install.packages("recommenderlab")
library(recommenderlab)
data("Jester5k")
Jester5k
as(Jester5k[10,],"list")
rowMeans(Jester5k[10,])
colMeans(Jester5k[,1])
hist(getRatings(Jester5k), breaks = 100)
hist(getRatings(normalize(Jester5k)), breaks = 100)
set.seed(2019)
e = evaluationScheme(Jester5k, method="split", train=0.8, given=15, goodRating = 5)
e
recommenderRegistry$get_entries(dataType="realRatingMatrix")
e
recommenderRegistry
recommenderRegistry$get_entries
recommenderRegistry$get_entries(dataType = "realRatingMatrix")
ubcf = Recommender(getData(e,"train"),"UBCF")
ibcf = Recommender(getData(e,"train"),"IBCF")
svd = Recommender(getData(e,"train"),"SVD")
popular = Recommender(getData(e,"train"),"POPULAR")
pca = Recommender(getData(e,"train"),"PCA")
set.seed(123)
e = evaluationScheme(Jester5k, method="split", train=0.8, given=15, goodRating = 5)
e
recommenderRegistry$get_entries(dataType = "realRatingMatrix")
df<-as(Jester5k, "data.frame")
str(df)
df
recommender_models <- recommenderRegistry$get_entries(dataType = "realRatingMatrix")
recommender_models
recommenderRegistry$get_entries(dataType = "realRatingMatrix")
ubcf = Recommender(getData(e,"train"),"UBCF")
pca = Recommender(getData(e,"train"),"PCA")
random = Recommender(getData(e,"train"),"RANDOM")
user_pred = predict(ubcf, getData(e,"known"),type="ratings")
ibcf_pred = predict(ibcf, getData(e,"known"),type="ratings")
user_pred = predict(ubcf, getData(e,"known"),type="ratings")
ibcf_pred = predict(ibcf, getData(e,"known"),type="ratings")
svd_pred = predict(svd, getData(e,"known"),type="ratings")
popular_pred = predict(popular, getData(e,"known"),type="ratings")
random_pred = predict(random, getData(e,"known"),type="ratings")
P1 = calcPredictionAccuracy(user_pred, getData(e,"unknown"))
P1
item_pred = predict(ibcf, getData(e,"known"),type="ratings")
svd_pred = predict(svd, getData(e,"known"),type="ratings")
popular_pred = predict(popular, getData(e,"known"),type="ratings")
random_pred = predict(random, getData(e,"known"),type="ratings")
P1 = calcPredictionAccuracy(user_pred, getData(e,"unknown"))
P1
P2 = calcPredictionAccuracy(item_pred, getData(e,"known"))
P2
ubcf = Recommender(getData(e,"train"),"UBCF")
ibcf = Recommender(getData(e,"train"),"IBCF")
svd = Recommender(getData(e,"train"),"SVD")
popular = Recommender(getData(e,"train"),"POPULAR")
random = Recommender(getData(e,"train"),"RANDOM")
user_pred = predict(ubcf, getData(e,"known"),type="ratings")
item_pred = predict(ibcf, getData(e,"known"),type="ratings")
svd_pred = predict(svd, getData(e,"known"),type="ratings")
popular_pred = predict(popular, getData(e,"known"),type="ratings")
random_pred = predict(random, getData(e,"known"),type="ratings")
P1 = calcPredictionAccuracy(user_pred, getData(e,"unknown"))
P1
P2 = calcPredictionAccuracy(item_pred, getData(e,"known"))
p2
P2
P1 = calcPredictionAccuracy(user_pred, getData(e,"unknown"))
P1
P2 = calcPredictionAccuracy(item_pred, getData(e,"known"))
P3 = calcPredictionAccuracy(item_pred, getData(e,"known"))
P4 = calcPredictionAccuracy(popular_pred, getData(e,"known"))
P5 = calcPredictionAccuracy(random_pred, getData(e,"known"))
error = rbind(P1,P2,P3,P4,P5)
rownames(error) = c("UBCF","IBCF","SVD","Popular","Random")
error
library(recommenderlab)
data(Jester5k)
r <- sample(Jester5k, 1000)
hist(getRatings(normalize(r, method="Z-score")), breaks=100)
r_POPULAR <- Recommender(r[1:100], method="POPULAR")
recom <- predict(r_POPULAR, r[250], n=3)
as(recom, "list")
scheme <- evaluationScheme(r[1:1000], method="split", train=0.7, k=1, given=-5, goodRating=5)
algorithms <- list("random items" = list(name="RANDOM", param=NULL), "popular items" = list(name="POPULAR", param=NULL), "user-based CF" = list(name="UBCF", param=list(nn=50)), "item-based CF" = list(name="IBCF", param=list(k=50)), "SVD approximation" = list(name="SVD", param=list(k=50)))
results <- evaluate(scheme, algorithms, type="topNList", n=c(1, 3, 5, 10, 15, 20))
plot(results, annotate=c(1,3), legend="bottomright")
plot(results, "prec/rec", annotate=3, legend="topleft")
e = evaluationScheme(Jester5k, method="split", train=0.7, k=1, given=15, goodRating=5)
r = sample(Jester5k, 1000)
hist(getRatings(normalize(r, method="Z-score")), breaks=100)
r_POPULAR = Recommender(r[1:100], method="POPULAR")
recom = predict(r_POPULAR, r[250], n=3)
as(recom, "list")
scheme = evaluationScheme(r[1:1000], method="split", train=0.7, k=1, given=-5, goodRating=5)
e = evaluationScheme(Jester5k, method="split", train=0.7, k=1, given=15, goodRating=5)
scheme
e
recommenderRegistry$get_entries(dataType = "realRatingMatrix")
scheme = evaluationScheme(r[1:1000], method="split", train=0.7, k=1, given=-5, goodRating=5)
recommenderRegistry$get_entries(dataType = "realRatingMatrix")
#순차 데이터 분석
getwd()
setwd( "D:/R")
#순차 데이터 분석
df = read.csv("sequential.csv")
str(df)
table(df$Cust_Segment)
table(df$Purchase1)
table(unlist(df[,-1]))
dfCount = count(df,Purchase1, Purchase2)
dfCount = count(df,Purchase1, Purchase2)
#순차 데이터 분석
library(dplyr)
dfCount = count(df,Purchase1, Purchase2)
dfCount = arrange(dfCount, desc(n))
dim(dfCount)
head(dfCount)
seq = seqdef(df[,-1], xtstep=1)
seq = seq.def(df[,-1], xtstep=1)
seq = seq.default(df[,-1], xtstep=1)
install.packages("TraMineR")
library(TraMineR)
df = read.csv("sequential.csv")
str(df)
table(df$Cust_Segment)
table(df$Purchase1)
table(unlist(df[,-1]))
dfCount = count(df,Purchase1, Purchase2)
dfCount = arrange(dfCount, desc(n))
dim(dfCount)
head(dfCount)
seq = seqdef(df[,-1], xtstep=1)
head(seq)
seqiplot(seq)
seqdplot(seq)
seqdplot(seq, group=df$Cust_Segment)
seqmsplot(seq, group = df$Cust_Segment)
seqmtplot(seq, group=df$Cust_Segment)
seqE = seqecreate(seq)
subSeq = seqefsub(seqE, pMinSupport = 0.05)
plot(subSeq[1:10], col="dodgerblue")
seqMat = seqtrate(seq)
options(digits = 2)
seqMat[2:4,1:3]
seqMat[,1]
