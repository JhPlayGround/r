#xgboost
library(xgboost)
train_x = as.matrix(train[,c(2:17)])
train_y = train[,1]
test_x = as.matrix(test[,c(2:17)])
test_y = test[,1]
xgb.train = xgb.DMatrix(data = train_x, label = train_y)
xgb.test =  xgb.DMatrix(data = test_x, label = test_y)
params = list(booster="gbtree",eta=0.001,max_depth=5,gamma=3,subsample=0.75,colsample_bytree=1,objective="reg:linear",eval_metric="rmse")
xgb.fit=xgb.train(params=params,data=xgb.train,nrounds=10000,nthreads=1,early_stopping_rounds=10,watchlist=list(val1=xgb.train,val2=xgb.test),verbose=0)
xgb.fit
Forecast = predict(xgb.fit, test_x,reshape=T)
Forecast  = round(Forecast)
Actual = test_y
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
mean((tmp$Actual-tmp$Forecast)^2) #52.70396
cor(Forecast, Actual)
source('D:/SUNMOON/2_Rwork/dust_semi.R', encoding = 'UTF-8')
source('D:/SUNMOON/2_Rwork/dust_semi.R', encoding = 'UTF-8')
getwd()
beijing_dust= read.csv("final_beijing_dust.csv")
head(beijing_dust)
names(beijing_dust) = c("date","ch_pm2.5")
head(beijing_dust)
View(beijing_dust)
head(beijing_dust,10)
beijing_climate= read.csv("beijing_climate.csv")
head(beijing_climate)
names(beijing_climate) = c("date","ch_avgtemp","ch_humidity","ch_rain","ch_sight","ch_wind")
head(beijing_climate,10)
seoul_dust= read.csv("seoul_dust.csv")
head(seoul_dust)
names(seoul_dust) = c("date","k_pm10","k_pm2.5","k_O3","k_NO2","k_CO","k_SO2")
head(seoul_dust)
head(seoul_dust,10)
seoul_climate= read.csv("seoul_climate.csv")
head(seoul_climate)
names(seoul_climate) = c("date","k_avgtemp","k_rain","k_wind","k_wind_direct","k_humidity")
head(seoul_climate,10)
library(dplyr)
beijing = left_join(beijing_dust,beijing_climate, by = "date")
beijing
seoul = left_join(seoul_dust,seoul_climate, by = "date")
seoul
seoul$date=gsub("-", "", seoul$date)
seoul$date = as.numeric(seoul$date)
data = left_join(seoul, beijing, by = "date")
head(data,10)
summary(data)
plot_missing(data)
plot_missing(data)
data$ch_rain = ifelse(is.na(data$ch_rain), 0, data$ch_rain)
data$k_rain = ifelse(is.na(data$k_rain), 0, data$k_rain)
data$ch_sight <- ifelse(is.na(data$ch_sight), round(mean(data$ch_sight,na.rm = T),2), data$ch_sight)
table(data$k_wind_direct)
data$k_wind_direct <- ifelse(is.na(data$k_wind_direct), 270, data$k_wind_direct)
data$k_wind <- ifelse(is.na(data$k_wind), round(mean(data$k_wind,na.rm = T),1), data$k_wind)
data$ch_wind <- ifelse(is.na(data$ch_wind), round(mean(data$ch_wind,na.rm = T),1), data$ch_wind)
data$ch_avgtemp <- ifelse(is.na(data$ch_avgtemp), round(mean(data$ch_avgtemp,na.rm = T),1), data$ch_avgtemp)
data$ch_humidity <- ifelse(is.na(data$ch_humidity), round(mean(data$ch_humidity,na.rm = T)), data$ch_humidity)
data$ch_pm2.5 <- ifelse(is.na(data$ch_pm2.5), round(mean(data$ch_pm2.5,na.rm = T),5), data$ch_pm2.5)
plot_missing(data)
summary(data)
#상관 분석
library(corrplot)
data.cor = cor(data)
data.cor
corrplot(data.cor)
#날짜 제거
data = data[,-1]
data
#상관 분석
library(corrplot)
data.cor = cor(data)
data.cor
corrplot(data.cor)
data.cor
#### 선형 회귀
library(leaps)
set.seed(2019)
index = sample(nrow(data), size = nrow(data) *0.7 , replace = F)
data
train = data[index,]#train : 1353개
train
str(train)
test = data[-index,] #test : 581개
test
str(test)
test_x = as.data.frame(test[,c(2:17)])
test_y = test[,1]
fit = lm(k_pm10 ~ ., data= train)
reduced.model=step(fit,direction="backward")
summary(reduced.model)
best.fit = lm(formula = k_pm10 ~ k_pm2.5 + k_NO2 + k_SO2 + k_avgtemp + k_wind +
k_wind_direct + k_humidity + ch_pm2.5 + ch_humidity + ch_sight,
data = train)
summary(best.fit)
library(car)
sqrt(vif(best.fit)) > 2
Actual = test_y
Actual
Forecast=predict(best.fit, newdata = test_x)
Forecast
Forecast = round(Forecast)
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
tmp$diff = tmp$Actual - tmp$Forecast
mean((tmp$Actual-tmp$Forecast)^2) # 61.81928
library(ggplot2)
ggplot(tmp, aes(x=Forecast, y=Actual)) + geom_point() + geom_smooth(method = lm) +labs(title = "Forecast versus Actuals - Linear Regression")
cor(tmp$Actual, tmp$Forecast)
getwd()
beijing_dust= read.csv("final_beijing_dust.csv")
head(beijing_dust)
names(beijing_dust) = c("date","ch_pm2.5")
head(beijing_dust,10)
beijing_climate= read.csv("beijing_climate.csv")
head(beijing_climate)
names(beijing_climate) = c("date","ch_avgtemp","ch_humidity","ch_rain","ch_sight","ch_wind")
head(beijing_climate,10)
seoul_dust= read.csv("seoul_dust.csv")
head(seoul_dust)
names(seoul_dust) = c("date","k_pm10","k_pm2.5","k_O3","k_NO2","k_CO","k_SO2")
head(seoul_dust,10)
seoul_climate= read.csv("seoul_climate.csv")
head(seoul_climate)
names(seoul_climate) = c("date","k_avgtemp","k_rain","k_wind","k_wind_direct","k_humidity")
head(seoul_climate,10)
library(dplyr)
beijing = left_join(beijing_dust,beijing_climate, by = "date")
beijing
seoul = left_join(seoul_dust,seoul_climate, by = "date")
seoul
seoul$date=gsub("-", "", seoul$date)
seoul$date = as.numeric(seoul$date)
data = left_join(seoul, beijing, by = "date")
head(data,10)
summary(data)
#결측치 제거
library(DataExplorer)
plot_missing(data)
data$ch_rain = ifelse(is.na(data$ch_rain), 0, data$ch_rain)
data$k_rain = ifelse(is.na(data$k_rain), 0, data$k_rain)
data$ch_sight <- ifelse(is.na(data$ch_sight), round(mean(data$ch_sight,na.rm = T),2), data$ch_sight)
table(data$k_wind_direct)
data$k_wind_direct <- ifelse(is.na(data$k_wind_direct), 270, data$k_wind_direct)
data$k_wind <- ifelse(is.na(data$k_wind), round(mean(data$k_wind,na.rm = T),1), data$k_wind)
data$ch_wind <- ifelse(is.na(data$ch_wind), round(mean(data$ch_wind,na.rm = T),1), data$ch_wind)
data$ch_avgtemp <- ifelse(is.na(data$ch_avgtemp), round(mean(data$ch_avgtemp,na.rm = T),1), data$ch_avgtemp)
data$ch_humidity <- ifelse(is.na(data$ch_humidity), round(mean(data$ch_humidity,na.rm = T)), data$ch_humidity)
data$ch_pm2.5 <- ifelse(is.na(data$ch_pm2.5), round(mean(data$ch_pm2.5,na.rm = T),5), data$ch_pm2.5)
plot_missing(data)
summary(data)
#이상치 제거
data$k_pm10 = ifelse(data$k_pm10>=93, 93, data$k_pm10)
data$k_pm2.5 = ifelse(data$k_pm2.5>=52, 52, data$k_pm2.5)
data$k_O3 = ifelse(data$k_O3>=0.060, 0.060, data$k_O3)
data$k_NO2 = ifelse(data$k_NO2>=0.063, 0.063, data$k_NO2)
data$k_CO = ifelse(data$k_CO>=0.8, 0.8, data$k_CO)
data$k_SO2 = ifelse(data$k_SO2>=0.006, 0.006, data$k_SO2)
data$k_SO2 = ifelse(data$k_SO2<=0.003, 0.003, data$k_SO2)
data$k_wind = ifelse(data$k_wind>=3.9, 3.9, data$k_wind)
data$ch_pm2.5 = ifelse(data$ch_pm2.5>=128.48042, 128.48042, data$ch_pm2.5)
data$ch_wind = ifelse(data$ch_wind>=19.1, 19.1, data$ch_wind)
summary(data)
#날짜 제거
data = data[,-1]
data
#상관 분석
library(corrplot)
data.cor = cor(data)
data.cor
corrplot(data.cor)
#### 선형 회귀
library(leaps)
set.seed(2019)
index = sample(nrow(data), size = nrow(data) *0.7 , replace = F)
data
train = data[index,]#train : 1353개
train
str(train)
test = data[-index,] #test : 581개
test
str(test)
test_x = as.data.frame(test[,c(2:17)])
test_y = test[,1]
fit = lm(k_pm10 ~ ., data= train)
reduced.model=step(fit,direction="backward")
summary(reduced.model)
best.fit = lm(formula = k_pm10 ~ k_pm2.5 + k_NO2 + k_SO2 + k_avgtemp + k_wind +
k_wind_direct + k_humidity + ch_pm2.5 + ch_humidity + ch_sight,
data = train)
summary(best.fit)
library(car)
sqrt(vif(best.fit)) > 2
Actual = test_y
Actual
Forecast=predict(best.fit, newdata = test_x)
Forecast
Forecast = round(Forecast)
summary(best.fit)
library(car)
sqrt(vif(best.fit)) > 2
Actual = test_y
Actual
Forecast=predict(best.fit, newdata = test_x)
Forecast
Forecast = round(Forecast)
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
tmp$diff = tmp$Actual - tmp$Forecast
mean((tmp$Actual-tmp$Forecast)^2) # 61.81928
mean((tmp$Actual-tmp$Forecast)^2)
mean((tmp$Actual-tmp$Forecast)^2)
cor(tmp$Actual, tmp$Forecast)
ggplot(tmp, aes(x=Forecast, y=Actual)) + geom_point() + geom_smooth(method = lm) +labs(title = "Forecast versus Actuals - Linear Regression")
relweights <- function(fit,...){
R <- cor(fit$model)
nvar <- ncol(R)
rxx <- R[2:nvar, 2:nvar]
rxy <- R[2:nvar, 1]
svd <- eigen(rxx)
evec <- svd$vectors
ev <- svd$values
delta <- diag(sqrt(ev))
lambda <- evec %*% delta %*% t(evec)
lambdasq <- lambda ^ 2
beta <- solve(lambda) %*% rxy
rsquare <- colSums(beta ^ 2)
rawwgt <- lambdasq %*% beta ^ 2
import <- (rawwgt / rsquare) * 100
import <- as.data.frame(import)
row.names(import) <- names(fit$model[2:nvar])
names(import) <- "Weights"
import <- import[order(import),1, drop=FALSE]
dotchart(import$Weights, labels=row.names(import),
xlab="% of R-Square", pch=19,
main="Relative Importance of Predictor Variables",
sub=paste("Total R-Square=", round(rsquare, digits=3)),
...)
return(import)
}
relweights(best.fit, col="black")
#의사 결정 나무
library(rpart)
my.control<-rpart.control(xval=10,cp=0,minsplit=nrow(train)*0.05)
tree.fit<-rpart(k_pm10~.,data=train,method='anova',control=my.control)
printcp(tree.fit)
which.min(tree.fit$cptable[,'xerror'])
prune.tree.fit = prune(tree.fit,cp=tree.fit$cptable[32])
prune.tree.fit = prune(tree.fit,cp=tree.fit$cptable[31])
prune.tree.fit
summary(prune.tree.fit)
prune.tree.fit$variable.importance
plot(prune.tree.fit,uniform=T,margin=0.1)
text(prune.tree.fit,use.n=T,col='blue',cex=0.8)
Actual = test_y
Forecast<-predict(prune.tree.fit,newdata=test_x,type='vector')
Forecast = round(Forecast)
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
mean((tmp$Actual-tmp$Forecast)^2) # 74.60241
prune.tree.fit$variable.importance
plot(prune.tree.fit$variable.importance)
cor(tmp$Actual,tmp$Forecast)
summary(best.fit)
#랜덤 포레스트
library(randomForest)
rf.fit<-randomForest(k_pm10~.,data=train,ntree=100,mtry=5,importance=T,na.action=na.omit)
rf.fit
my.control<-rpart.control(xval=10,cp=0,minsplit=nrow(train)*0.05)
tree.fit<-rpart(k_pm10~.,data=train,method='anova',control=my.control)
printcp(tree.fit)
rf.fit<-randomForest(k_pm10~.,data=train,ntree=100,mtry=5,importance=T,na.action=na.omit)
rf.fit
importance(rf.fit,type=1)
Actual = test_y
Forecast = predict(rf.fit,newdata=test_x)
Forecast = round(Forecast)
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
mean((tmp$Actual-tmp$Forecast)^2) # 60.57143
cor(tmp$Actual, tmp$Forecast)
ggplot(tmp, aes(x=Forecast, y=Actual)) + geom_point() + geom_smooth(method = lm) +labs(title = "Forecast versus Actuals - RandomForest")
#xgboost
library(xgboost)
train_x = as.matrix(train[,c(2:17)])
train_y = train[,1]
test_x = as.matrix(test[,c(2:17)])
test_y = test[,1]
xgb.train = xgb.DMatrix(data = train_x, label = train_y)
xgb.test =  xgb.DMatrix(data = test_x, label = test_y)
params = list(booster="gbtree",eta=0.001,max_depth=5,gamma=3,subsample=0.75,colsample_bytree=1,objective="reg:linear",eval_metric="rmse")
xgb.fit=xgb.train(params=params,data=xgb.train,nrounds=10000,nthreads=1,early_stopping_rounds=10,watchlist=list(val1=xgb.train,val2=xgb.test),verbose=0)
xgb.fit
Forecast = predict(xgb.fit, test_x,reshape=T)
Forecast  = round(Forecast)
Actual = test_y
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
mean((tmp$Actual-tmp$Forecast)^2) #52.97418
cor(tmp$Actual,tmp$Forecast)
importance_matrix
xgb.plot.importance(importance_matrix = importance_matrix)
my.control<-rpart.control(xval=10,cp=0,minsplit=nrow(train)*0.05)
tree.fit<-rpart(k_pm10~.,data=train,method='anova',control=my.control)
printcp(tree.fit)
which.min(tree.fit$cptable[,'xerror'])
prune.tree.fit = prune(tree.fit,cp=tree.fit$cptable[31])
prune.tree.fit
summary(prune.tree.fit)
prune.tree.fit$variable.importance
plot(prune.tree.fit,uniform=T,margin=0.1)
text(prune.tree.fit,use.n=T,col='blue',cex=0.8)
Actual = test_y
Forecast<-predict(prune.tree.fit,newdata=test_x,type='vector')
Forecast = round(Forecast)
#### 선형 회귀
library(leaps)
set.seed(2019)
index = sample(nrow(data), size = nrow(data) *0.7 , replace = F)
data
train = data[index,]#train : 1353개
train
str(train)
test = data[-index,] #test : 581개
test
str(test)
test_x = as.data.frame(test[,c(2:17)])
test_y = test[,1]
fit = lm(k_pm10 ~ ., data= train)
my.control<-rpart.control(xval=10,cp=0,minsplit=nrow(train)*0.05)
tree.fit<-rpart(k_pm10~.,data=train,method='anova',control=my.control)
printcp(tree.fit)
which.min(tree.fit$cptable[,'xerror'])
prune.tree.fit = prune(tree.fit,cp=tree.fit$cptable[31])
prune.tree.fit
summary(prune.tree.fit)
prune.tree.fit$variable.importance
plot(prune.tree.fit,uniform=T,margin=0.1)
text(prune.tree.fit,use.n=T,col='blue',cex=0.8)
Actual = test_y
Forecast<-predict(prune.tree.fit,newdata=test_x,type='vector')
Forecast = round(Forecast)
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
mean((tmp$Actual-tmp$Forecast)^2) # 74.60241
cor(tmp$Actual,tmp$Forecast)
ggplot(tmp, aes(x=Forecast, y=Actual)) + geom_point() + geom_smooth(method = lm) +labs(title = "Forecast versus Actuals - Decision Tree")
train_x = as.matrix(train[,c(2:17)])
train_y = train[,1]
test_x = as.matrix(test[,c(2:17)])
test_y = test[,1]
xgb.train = xgb.DMatrix(data = train_x, label = train_y)
xgb.test =  xgb.DMatrix(data = test_x, label = test_y)
params = list(booster="gbtree",eta=0.001,max_depth=5,gamma=3,subsample=0.75,colsample_bytree=1,objective="reg:linear",eval_metric="rmse")
xgb.fit
Forecast = predict(xgb.fit, test_x,reshape=T)
Forecast  = round(Forecast)
Actual = test_y
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
mean((tmp$Actual-tmp$Forecast)^2) #52.97418
cor(tmp$Actual,tmp$Forecast)
ggplot(tmp, aes(x=Forecast, y=Actual)) + geom_point() + geom_smooth(method = lm) +labs(title = "Forecast versus Actuals - xgboost")
source('D:/SUNMOON/2_Rwork/dust_semi.R', encoding = 'UTF-8')
library(cluster)
library(compareGroups)
install.packages("compareGroups")
library("HDclassif")
install.packages("HDclassif")
library(HDclassif)
install.packages("NbClust")
library(NbClust) #군집 유효성 측정
install.packages("sparcl")
library(sparcl)
data(wine)
str(wine)
names(wine) = c("Class","Alchol","MalicAcid","Ash","Alk_ash","magnesium","T_phenols","Flavanoids","Non_flav",
"Proantho","C_Intensity","Hue","0D280_315","Proline")
names(wine)
df = as.data.frame(scale(wine[,-1]))
str(df)
table(wine$Class)
numComplete = NbClust(df, distance = "euclidean", min.nc = 2, max.nc = 6, method="complete",index="all")
numComplete$Best.nc
dis = dist(df, method="euclidean")
hc = hclust(dis, method="complete")
plot(hc, hang = -1, labels = F, main="Complete-Linkage")
com3 = cutree(hc,3)
ColorDendrogram(hc, y=comp3, main="Complete", branchlength = 50)
com3 = cutree(hc,3)
ColorDendrogram(hc, y=comp3, main="Complete", branchlength = 50)
comp3 = cutree(hc,3)
ColorDendrogram(hc, y=comp3, main="Complete", branchlength = 50)
table(comp3)
table(comp3, wine$Class)
numWard = NbClust(df, diss=NULL, distance = "euclidean", min.nc = 2, max.nc = 6, method="ward.D2", index="all")
hcWard = hclust(dis, method = "ward.D2")
plot(hcWard, labels = F, main="Ward's - Linkage")
ward3 = cutree(hcWard,3)
table(ward3, wine$Class)
table(comp3, ward3)
aggregate(wine[,-1], list(comp3), mean)
aggregate(wine[,-1],list(ward3), mean)
par(mfrow=c(1,2))
boxplot(wine$Proline ~ comp3, data=wine, main="Proline by Complete Linkage")
boxplot(wine$Proline ~ ward3, data=wine, main = "Proline by Ward's Linkage")
#kmeans 군집화
numKMeans = NbClust(df,min.nc = 2, max.nc = 15, method="kmeans")
set.seed(2019)
km = kmeans(df, 3, nstart = 25)
table(km$cluster)
km$centers
boxplot(wine$Alchol ~ km$cluster, data = wine, main = "Alchol Content,K-Means")
boxplot(wine$Alchol ~ ward3, data = wine, main="Alchol Content, Ward's")
table(km$cluster, wine$Class)
wine$Alchol = as.factor(ifelse(df$Alchol > 0),"Hight","Low")
wine$Alchol = as.factor(ifelse(df$Alchol > 0,"Hight","Low"))
disMatrix = daisy(wine[,-1],metric = "gower")
set.seed(2019)
pamFit = pam(disMatrix, k=3)
table(km$cluster, wine$Class)
table(pamFit$clustering)
table(pamFit$clustering, wine$Class)
wine$Cluster = pamFit$clustering
group = compareGroups(cluster ~ ., data=wine)
library(compareGroups)
group = compareGroups(cluster ~ ., data=wine)
group = compareGroups(cluster ~ ., data =wine)
group = compareGroups(Cluster ~ ., data =wine)
clustab = createTable(group)
clustab
export2csv(clustab, file="wine_clusters.csv")
#랜덤 포르세트 , 중간점 구분 분할
sset.seed(2019)
#랜덤 포르세트 , 중간점 구분 분할
set.seed(2019)
library(randomForest)
install.packages("randomForest")
install.packages("randomForest")
library(randomForest)
rf = randomForest(x = wine[,-1], ntree = 200, proximity = T)
rf
dim(rf$proximity)
rf$proximity[1:5,1:5]
importance(rf)
dissMat = sqrt(1 - rf$proximity)
dissMat[1:2, 1:2]
set.seed(2019)
pamRF = pam(dissMat, k=3)
table(pamRF$clustering)
table(pamRF$clustering, wine$Class)
library(ggplot2)
getwd()
setwd("D:/R")
getwd()
train = read.csv("NHLtrain.csv")
str(train)
names(train)
train.scale = scale(train[,-1:-2])
nhl.cor = cor(train.scale)
cor.plot(nhl.cor)
library(cor)
library(corrplot)
cor.plot(nhl.cor)
corplot(nhl.cor)
corPlot(nhl.cor)
corrplot(nhl.cor)
#성분추출
pca = principal(train.scale, rotate = "none")
library(psych) #pca
library(ggplot2)
library(psych) #pca
library(corrplot)
#성분추출
pca = principal(train.scale, rotate = "none")
plot(pca$values, type="b", yylab = "Eigenvalues", xlab ="Component")
plot(pca$values, type="b", ylab = "Eigenvalues", xlab ="Component")
#직각 회전, 해석
pca.rotate = principal(train.scale, nfactors = 5, rotate = "varimax")
pca.rotate
#성분으로부터 요인 점수 생성
pca.scores = data.frame(pca.rotate$scores)
head(pca.scores)
pca.scores$ppg = train$ppg
#회귀 분석
nhl.lm = lm(ppg ~., data= pca.scores)
summary(nhl.lm)
nhl.lm2 = lm(ppg ~ RC1 + RC2, data = pca.scores)
summary(nhl.lm2)
plot(nhl.lm2$fitted.values, train$ppg, main="Predicted VS Actual", xlab="Predicted", ylab="Actual")
train$pred = round(nhl.lm2$fitted.values, digits = 2)
p = ggplot(train , aes(x=pred, y= ppg, label=Team))
p + geom_point() + geom_text(size=3.5, hjust = 0.1, vjust =-0.5, angle=0) + xlim(0.8,1.4) + ylim(0.8,1.5)+
stat_smooth(method = "lm", se=F)
pca.scores$Team = train$Team
p2 = ggplot(pca.scores, aes(x = RC1, y = RC2, label=Team))
p2 + geom_point() + geom_text(size=2.75, hjust=0.2, vjust=-0.75, angle=0) + xlim(-2.5,2.5) + ylim(-3.0,2.5)
sqrt(mean(nhl.lm2$residuals^2))
test = read.csv("NHLtest.csv")
test.scores = data.frame(predict(pca.rotate, test[,c[-1:-2]]))
test.scores = data.frame(predict(pca.rotate, test[,c(-1:-2)]))
test.scores$pred = predict(nhl.lm2, test.scores)
test.scores$ppg = test$ppg
test.scores$Team = test$Team
p3 = ggplot(test.scores, aes(x=pred, y=ppg, label=Team))
p3 + geom_point() + geom_text(size=3.5, hjust=0.4, vjust=-0.9,angle=35)+xlim(0.75,1.5) + ylim(0.5,1.6)+
stat_smooth(method = "lm", se=F)
resid = test.scores$ppg - test.scores$pred
sqrt(mean(resid^2))
