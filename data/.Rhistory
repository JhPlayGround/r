rf.fit
importance(rf.fit,type=1)
Actual = test_y
Forecast = predict(rf.fit,newdata=test_x)
Forecast = round(Forecast)
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
#모델 검증
mse(tmp$Actual, tmp$Forecast) #61.48365
rmse(tmp$Actual, tmp$Forecast) #7.841151
mape(tmp$Actual, tmp$Forecast) # 0.1282503
mase(tmp$Actual, tmp$Forecast,1) #0.3128823
cor(tmp$Actual, tmp$Forecast) #0.9177493
#시각화
ggplot = ggplot(tmp, aes(x=Forecast, y=Actual)) + geom_point() + geom_smooth(method = lm) +labs(title = "Forecast versus Actuals - RandomForest")
ggplot
ggsave("randomForest.png",plot = ggplot,width = 10, height = 5)
#xgboost
library(xgboost)
set.seed(2019)
index = sample(nrow(data), size = nrow(data) *0.7 , replace = F)
data
train = data[index,]#train : 1353개
train
str(train)
test = data[-index,] #test : 581개
test
str(test)
train_x = as.matrix(train[,c(2:22)])
train_y = train[,1]
test_x = as.matrix(test[,c(2:22)])
test_y = test[,1]
xgb.train = xgb.DMatrix(data = train_x, label = train_y)
xgb.test =  xgb.DMatrix(data = test_x, label = test_y)
params = list(booster="gbtree",eta=0.001,max_depth=5,gamma=3,subsample=0.75,colsample_bytree=1,objective="reg:linear",eval_metric="rmse")
xgb.fit=xgb.train(params=params,data=xgb.train,nrounds=10000,nthreads=1,early_stopping_rounds=10,watchlist=list(val1=xgb.train,val2=xgb.test),verbose=0)
xgb.fit
Forecast = predict(xgb.fit, test_x,reshape=T)
Forecast  = round(Forecast)
Actual = test_y
tmp = cbind(Actual,Forecast )
tmp = as.data.frame(tmp)
#모델 검증
mse(tmp$Actual, tmp$Forecast) #52.18933
rmse(tmp$Actual, tmp$Forecast) #7.224218
mape(tmp$Actual, tmp$Forecast) #0.09937504
mase(tmp$Actual, tmp$Forecast,1) #0.2671256
cor(tmp$Actual, tmp$Forecast) #0.9287739
ggplot=ggplot(tmp, aes(x=Forecast, y=Actual)) + geom_point() + geom_smooth(method = lm) +labs(title = "Forecast versus Actuals - xgboost")
ggplot
ggsave("xgboost.png",plot = ggplot,width = 10, height = 5)
importance_matrix <- xgb.importance(model = xgb.fit)
importance_matrix
xgb.plot.importance(importance_matrix = importance_matrix)
ggsave("xgboost_importance.png",plot = ggplot,width = 10, height = 5)
ggplot = xgb.plot.importance(importance_matrix = importance_matrix)
ggsave("xgboost_importance.png",plot = ggplot,width = 10, height = 5)
png("importance_matrix.png",width = 2000, height = 1500)
ggplot = xgb.plot.importance(importance_matrix = importance_matrix)
dev.off()
getwd()
beijing_dust= read.csv("final_beijing_dust.csv")
head(beijing_dust)
names(beijing_dust) = c("date","ch_pm2.5")
head(beijing_dust,10)
beijing_climate= read.csv("beijing_climate.csv")
head(beijing_climate)
names(beijing_climate) = c("date","ch_avgtemp","ch_humidity","ch_rain","ch_sight","ch_wind")
head(beijing_climate,10)
seoul_dust= read.csv("seoul_dust.csv")
head(seoul_dust)
names(seoul_dust) = c("date","k_pm10","k_pm2.5","k_O3","k_NO2","k_CO","k_SO2")
head(seoul_dust,10)
seoul_climate= read.csv("seoul_climate.csv")
head(seoul_climate)
names(seoul_climate) = c("date","k_avgtemp","k_rain","k_wind","k_wind_direct","k_humidity")
head(seoul_climate,10)
library(dplyr)
beijing = left_join(beijing_dust,beijing_climate, by = "date")
beijing
seoul = left_join(seoul_dust,seoul_climate, by = "date")
seoul
seoul$date=gsub("-", "", seoul$date)
seoul$date = as.numeric(seoul$date)
data = left_join(seoul, beijing, by = "date")
head(data,10)
summary(data)
#결측치 제거
library(DataExplorer)
plot_missing(data)
data$ch_rain = ifelse(is.na(data$ch_rain), 0, data$ch_rain)
data$k_rain = ifelse(is.na(data$k_rain), 0, data$k_rain)
data$ch_sight <- ifelse(is.na(data$ch_sight), round(mean(data$ch_sight,na.rm = T),2), data$ch_sight)
table(data$k_wind_direct)
data$k_wind_direct <- ifelse(is.na(data$k_wind_direct), 270, data$k_wind_direct)
data$k_wind <- ifelse(is.na(data$k_wind), round(mean(data$k_wind,na.rm = T),1), data$k_wind)
data$ch_wind <- ifelse(is.na(data$ch_wind), round(mean(data$ch_wind,na.rm = T),1), data$ch_wind)
data$ch_avgtemp <- ifelse(is.na(data$ch_avgtemp), round(mean(data$ch_avgtemp,na.rm = T),1), data$ch_avgtemp)
data$ch_humidity <- ifelse(is.na(data$ch_humidity), round(mean(data$ch_humidity,na.rm = T)), data$ch_humidity)
data$ch_pm2.5 <- ifelse(is.na(data$ch_pm2.5), round(mean(data$ch_pm2.5,na.rm = T),5), data$ch_pm2.5)
plot_missing(data)
summary(data)
#이상치 제거
data$k_pm10 = ifelse(data$k_pm10>=93, 93, data$k_pm10)
data$k_pm2.5 = ifelse(data$k_pm2.5>=52, 52, data$k_pm2.5)
data$k_O3 = ifelse(data$k_O3>=0.060, 0.060, data$k_O3)
data$k_NO2 = ifelse(data$k_NO2>=0.063, 0.063, data$k_NO2)
data$k_CO = ifelse(data$k_CO>=0.8, 0.8, data$k_CO)
data$k_SO2 = ifelse(data$k_SO2>=0.006, 0.006, data$k_SO2)
data$k_SO2 = ifelse(data$k_SO2<=0.003, 0.003, data$k_SO2)
data$k_wind = ifelse(data$k_wind>=3.9, 3.9, data$k_wind)
data$ch_pm2.5 = ifelse(data$ch_pm2.5>=128.48042, 128.48042, data$ch_pm2.5)
data$ch_wind = ifelse(data$ch_wind>=19.1, 19.1, data$ch_wind)
summary(data)
#이상치 제거
boxplot(data$k_pm10)$stat
#이상치 제거
data$k_pm10 = ifelse(data$k_pm10<=28, 28, data$k_pm10)
summary(data)
#이상치 제거
boxplot(data$k_humidity)
quantile(data$k_pm10, 3/4)
getwd()
beijing_dust= read.csv("final_beijing_dust.csv")
head(beijing_dust)
names(beijing_dust) = c("date","ch_pm2.5")
head(beijing_dust,10)
beijing_climate= read.csv("beijing_climate.csv")
head(beijing_climate)
names(beijing_climate) = c("date","ch_avgtemp","ch_humidity","ch_rain","ch_sight","ch_wind")
head(beijing_climate,10)
seoul_dust= read.csv("seoul_dust.csv")
head(seoul_dust)
names(seoul_dust) = c("date","k_pm10","k_pm2.5","k_O3","k_NO2","k_CO","k_SO2")
head(seoul_dust,10)
seoul_climate= read.csv("seoul_climate.csv")
head(seoul_climate)
names(seoul_climate) = c("date","k_avgtemp","k_rain","k_wind","k_wind_direct","k_humidity")
head(seoul_climate,10)
library(dplyr)
beijing = left_join(beijing_dust,beijing_climate, by = "date")
beijing
seoul = left_join(seoul_dust,seoul_climate, by = "date")
seoul
seoul$date=gsub("-", "", seoul$date)
seoul$date = as.numeric(seoul$date)
data = left_join(seoul, beijing, by = "date")
head(data,10)
summary(data)
#결측치 제거
library(DataExplorer)
plot_missing(data)
data$ch_rain = ifelse(is.na(data$ch_rain), 0, data$ch_rain)
data$k_rain = ifelse(is.na(data$k_rain), 0, data$k_rain)
data$ch_sight <- ifelse(is.na(data$ch_sight), round(mean(data$ch_sight,na.rm = T),2), data$ch_sight)
table(data$k_wind_direct)
data$k_wind_direct <- ifelse(is.na(data$k_wind_direct), 270, data$k_wind_direct)
data$k_wind <- ifelse(is.na(data$k_wind), round(mean(data$k_wind,na.rm = T),1), data$k_wind)
data$ch_wind <- ifelse(is.na(data$ch_wind), round(mean(data$ch_wind,na.rm = T),1), data$ch_wind)
data$ch_avgtemp <- ifelse(is.na(data$ch_avgtemp), round(mean(data$ch_avgtemp,na.rm = T),1), data$ch_avgtemp)
data$ch_humidity <- ifelse(is.na(data$ch_humidity), round(mean(data$ch_humidity,na.rm = T)), data$ch_humidity)
data$ch_pm2.5 <- ifelse(is.na(data$ch_pm2.5), round(mean(data$ch_pm2.5,na.rm = T),5), data$ch_pm2.5)
plot_missing(data)
summary(data)
#이상치 제거
data$k_pm10 = ifelse(data$k_pm10>=93, 93, data$k_pm10)
data$k_pm2.5 = ifelse(data$k_pm2.5>=52, 52, data$k_pm2.5)
data$k_O3 = ifelse(data$k_O3>=0.060, 0.060, data$k_O3)
data$k_NO2 = ifelse(data$k_NO2>=0.063, 0.063, data$k_NO2)
data$k_CO = ifelse(data$k_CO>=0.8, 0.8, data$k_CO)
data$k_SO2 = ifelse(data$k_SO2>=0.006, 0.006, data$k_SO2)
data$k_SO2 = ifelse(data$k_SO2<=0.003, 0.003, data$k_SO2)
data$k_wind = ifelse(data$k_wind>=3.9, 3.9, data$k_wind)
data$ch_pm2.5 = ifelse(data$ch_pm2.5>=128.48042, 128.48042, data$ch_pm2.5)
data$ch_wind = ifelse(data$ch_wind>=19.1, 19.1, data$ch_wind)
summary(data)
str(data)
getwd()
beijing_dust= read.csv("final_beijing_dust.csv")
head(beijing_dust)
names(beijing_dust) = c("date","ch_pm2.5")
head(beijing_dust,10)
beijing_climate= read.csv("beijing_climate.csv")
head(beijing_climate)
names(beijing_climate) = c("date","ch_avgtemp","ch_humidity","ch_rain","ch_sight","ch_wind")
head(beijing_climate,10)
seoul_dust= read.csv("seoul_dust.csv")
head(seoul_dust)
names(seoul_dust) = c("date","k_pm10","k_pm2.5","k_O3","k_NO2","k_CO","k_SO2")
head(seoul_dust,10)
seoul_climate= read.csv("seoul_climate.csv")
head(seoul_climate)
names(seoul_climate) = c("date","k_avgtemp","k_rain","k_wind","k_wind_direct","k_humidity")
head(seoul_climate,10)
library(dplyr)
beijing = left_join(beijing_dust,beijing_climate, by = "date")
beijing
seoul = left_join(seoul_dust,seoul_climate, by = "date")
seoul
seoul$date=gsub("-", "", seoul$date)
seoul$date = as.numeric(seoul$date)
data = left_join(seoul, beijing, by = "date")
head(data,10)
summary(data)
#결측치 제거
library(DataExplorer)
plot_missing(data)
data$ch_rain = ifelse(is.na(data$ch_rain), 0, data$ch_rain)
data$k_rain = ifelse(is.na(data$k_rain), 0, data$k_rain)
data$ch_sight <- ifelse(is.na(data$ch_sight), round(mean(data$ch_sight,na.rm = T),2), data$ch_sight)
table(data$k_wind_direct)
data$k_wind_direct <- ifelse(is.na(data$k_wind_direct), 270, data$k_wind_direct)
data$k_wind <- ifelse(is.na(data$k_wind), round(mean(data$k_wind,na.rm = T),1), data$k_wind)
data$ch_wind <- ifelse(is.na(data$ch_wind), round(mean(data$ch_wind,na.rm = T),1), data$ch_wind)
data$ch_avgtemp <- ifelse(is.na(data$ch_avgtemp), round(mean(data$ch_avgtemp,na.rm = T),1), data$ch_avgtemp)
data$ch_humidity <- ifelse(is.na(data$ch_humidity), round(mean(data$ch_humidity,na.rm = T)), data$ch_humidity)
data$ch_pm2.5 <- ifelse(is.na(data$ch_pm2.5), round(mean(data$ch_pm2.5,na.rm = T),5), data$ch_pm2.5)
plot_missing(data)
summary(data)
#이상치 제거
boxplot(data$k_pm10)
#이상치 제거
boxplot(data$k_pm10)$stat
quantile(data$k_pm10, 2/4)
quantile(data$k_pm10, 3/4)
quantile(data$k_pm10, 4/4)
quantile(data$k_pm10, 1/4)
library(MASS)
install.packages("caretEnsemble")
library(CaTools)
library("caTools")
install.packages("caTools")
install.packages("caTools")
library(caTools)
pima = rbind(Pima.tr, Pima.te)
set.seed(2019)
split = createDataPartition(y=pima$type, p = 0.7, list=F)
library(caretEnsemble)
split = createDataPartition(y=pima$type, p = 0.7, list=F)
library(caret)
split = createDataPartition(y=pima$type, p = 0.7, list=F)
train = pima[split,]
test = pima[-split,]
table(train$type)
control = trainControl(method="cv", number=5, savePredictions = "final",
classProbs = T, index=createResample(train$type,5),
sampling = "up", summaryFunction = towClassSummary)
control = trainControl(method="cv", number=5, savePredictions = "final",
classProbs = T, index=createResample(train$type,5),
sampling = "up", summaryFunction = twoClassSummary)
set.seed(2019)
models = createList(type ~., data = train, trControl = control, metric="ROC",
methodList = c("rpart","earth","knn"))
models = caretList(type ~., data = train, trControl = control, metric="ROC",
methodList = c("rpart","earth","knn"))
models
modelCor(resamples(models))
models_preds = lapply(models, predict, newdata=test, type="prob")
modles_preds = lapply(models_preds, function(x) x[,"Yes"])
models_preds = lapply(models, predict, newdata=test, type="prob")
models_preds = lapply(models_preds, function(x) x[,"Yes"])
models_preds = data.frame(models_preds)
stack = caretStack(models, method="glm",metric="ROC",
trControl = trainControl(method = "boot",number=5,
savePredictions = "final",classProbs = T,summaryFunction = twoClassSummary))
summary(stack)
prob = 1 - predict(stack, newdata = test, type="prob")
model_preds$ensemble = prob
models_preds$ensemble = prob
colAUC(models_preds, test$type)
library(mlr)
install.packages("mlr")
library(ggplot2)
library(HDclassif)
install.packages("DMwR")
library(DMwR)
library(reshape2)
library(corrplot)
data(wine)
table(wine$class)
wine$class = as.factor(wine$class)
set.seed(2019)
df = SMOTE(class ~., wine, perc.over = 300, prec.under = 300)
table(df$class)
wine.scale = data.frame(scale(wine[,2:5]))
wine.scale$class = wine$class
wine.melt = melt(wine.scale, id.vars = "class")
ggplot(wine.melt, aes(x=class, y=value)) + geom_boxplot() + facet_wrap( ~ variable, ncol=2)
#특이점 찾음
outHigh = function(x){
x[x > quantile(x, 0.99)] = quantile(x,-.75)
x
}
outLow = function(x){
x[x < quantile(x,0.01)] = quantile(x,0.25)
x
}
wine.trunc = data.frame(lapply(wine[,-1]),outHigh)
wine.trunc = data.frame(lapply(wine[,-1]),outHigh)
wine.trunc = data.frame(lapply(wine[,-1]),outHigh())
wine.trunc = data.frame(outHigh(lapply(wine[,-1])))
wine.trunc
wine.trunc = data.frame(lapply(wine[,-1],outHigh))
#특이점 찾음
outHigh = function(x){
x[x > quantile(x, 0.99)] = quantile(x,0.75)
x
}
outLow = function(x){
x[x < quantile(x,0.01)] = quantile(x,0.25)
x
}
wine.trunc = data.frame(lapply(wine[,-1],outHigh))
wine.trunc
wine.trunc = data.frame(lapply(wine.trunc, outLow))
wine.trunc$class = wine$class
ggplot(wine.trunc, aes(x=class, y=value)) + geom_boxplot() + facet_wrap( ~ variable, ncol=2)
wine.melt
wine.trunc
boxplot(wine.trunc$v3 ~ wine.trunc$class)
boxplot(wine.trunc$V3)
boxplot(wine.trunc$V3 ~ wine.trunc$class)
#상관 관계
c = cor(wine.trunc[,-14])
corrplot(c, uppper="ellipse")
corrplot(c, upper="ellipse")
corrplot.mixed(c, upper="ellipse")
library(caret)
set.seed(2019)
split = createDataPartition(y= df$class, p=0.7, list=F)
train = df[split,]
test = df[-split,]
wine.task = makeClassifTask(id="wine", data=train, target="class")
library(mlr)
wine.task = makeClassifTask(id="wine", data=train, target="class")
str(getTaskData(wine.task))
rdesc = makeResampleDesc("Subsample",iters=3)
param = makeParamSet(makeDiscreteParam("ntree",values=c(750,1000,1250,1500,1750,2000)))
ctrl = makeTuneControlGrid()
tuning = tuneParams("classif.randomForest",task=wine.task, resampling = rdesc, par.set = param,
control = ctrl)
tuning$x
tuning$y
rf= setHyperPars(makeLearner("classif.randomForest",predict.type="prob"), par.vals = tuning$x)
fitRF = train(rf, wine.task)
fitRF$learner.model
predRF = predict(fitRF, newdata = test)
getConfMatrix(predRF)
calculateConfusionMatrix(predRF)
performance(predRF, measures = list(mmce,acc))
ovr = makeMulticlassWrapper("classif.penalized.ridge",mcw.method = "onevsrest")
ovr = makeMulticlassWrapper("classif.penalized",mcw.method = "onevsrest")
ovr = makeMulticlassWrapper("classif.penalized.ridge",mcw.method = "onevsrest")
ovr = makeMulticlassWrapper("classif.penalized",mcw.method = "onevsrest")
ovr <- makeMulticlassWrapper("classif.penalized.ridge",       mcw.method = "onevsrest")
install.packages("penalized")
library(penalized)
ovr = makeMulticlassWrapper("classif.penalized.ridge",mcw.method = "onevsrest")
ovr <- makeMulticlassWrapper("classif.penalized.ridge",       mcw.method = "onevsrest")
ovr <- makeMulticlassWrapper("classif.penalized",       mcw.method = "onevsrest")
bag.ovr = makeBaggingWrapper(ovr, bw.iters = 10, bw.replace = T)
bag.ovr = makeBaggingWrapper(ovr, bw.iters = 10, bw.replace = T,
bw.size = 0.7, bw.feats = 1)
set.seed(2019)
fitOVR = mlr::train(bag.ovr, wine.task)
predOVR = predict(fitOVR, newdata=test)
head(data.frame(predOVR))
calculateConfusionMatrix(predOVR)
pima.task = makeClassifTask(id="pima", data=train, target="type")
pima
pima.task = makeClassifTask(id="pima", data=train, target="type")
names(pima)
pima.task = makeClassifTask(id="pima", data=train, target="type")
pima.task = makeClassifTask(id="pima", data=train)
pima.task = makeClassifTask(id="pima", data=train, target="glu")
names(pima) = names("npreg","glu","bp","skin","bmi","ped","age","type" )
names(pima) = c("npreg","glu","bp","skin","bmi","ped","age","type" )
pima.task = makeClassifTask(id="pima", data=train, target="glu")
pima.task = makeClassifTask(id="pima", data=train, target="type")
train
library(mlr)
library(ggplot2)
library(HDclassif)
library(DMwR)
library(reshape2)
library(corrplot)
data(wine)
table(wine$class)
wine$class = as.factor(wine$class)
set.seed(2019)
df = SMOTE(class ~., wine, perc.over = 300, prec.under = 300)
table(df$class)
wine.scale = data.frame(scale(wine[,2:5]))
wine.scale$class = wine$class
wine.melt = melt(wine.scale, id.vars = "class")
ggplot(wine.melt, aes(x=class, y=value)) + geom_boxplot() + facet_wrap( ~ variable, ncol=2)
#특이점 찾음 -> 제거
outHigh = function(x){
x[x > quantile(x, 0.99)] = quantile(x,0.75)
x
}
outLow = function(x){
x[x < quantile(x,0.01)] = quantile(x,0.25)
x
}
wine.trunc = data.frame(lapply(wine[,-1],outHigh))
wine.trunc = data.frame(lapply(wine.trunc, outLow))
wine.trunc$class = wine$class
boxplot(wine.trunc$V3 ~ wine.trunc$class)
#상관 관계
c = cor(wine.trunc[,-14])
corrplot.mixed(c, upper="ellipse")
library(mlr)
library(caret)
set.seed(2019)
split = createDataPartition(y= df$class, p=0.7, list=F)
train = df[split,]
test = df[-split,]
train
library(penalized)
ovr <- makeMulticlassWrapper("classif.penalized",       mcw.method = "onevsrest")
bag.ovr = makeBaggingWrapper(ovr, bw.iters = 10, bw.replace = T,
bw.size = 0.7, bw.feats = 1)
set.seed(2019)
fitOVR = mlr::train(bag.ovr, wine.task)
predOVR = predict(fitOVR, newdata=test)
head(data.frame(predOVR))
calculateConfusionMatrix(predOVR)
pima.task = makeClassifTask(id="pima", data=train, target="type")
pima
pima.task = makeClassifTask(id="pima", data=train, target="type")
library(tm)
library(wordcloud)
library(RColorBrewer)
getwd()
setwd("D:/r/data")
name = file.path("D:/r/data/")
length(dir(name))
dir(name)
docs = Corpus(DirSource(name))
docs
docs = tm_map(docs, tolower)
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeWords, stopwords("english"))
docs = tm_map(docs, stripWhitespace)
docs = tm_map(docs, removeWords, c("applause","can","cant","will",
"that","weve","dont","wont","youll","youre"))
dtm = DocumentTermMatrix(docs)
dim(dtm)
dtm = removeSparseTerms(dtm, 0.75)
dim(dtm)
rownames(dtm) = c("2010","2011","2012","2013","2014","2015","2016")
inspect(dtm[1:7,1:5])
freq = colSums(as.matrix(dtm))
ord = order(-freq)
freq[head(ord)]
freq[tail(ord)]
head(table(freq))
tail(table(freq))
findFreqTerms(dtm,125)
findAssocs(dtm,"jobs",corlimit = 0.85)
wordcloud(names(freq),freq, min.freq = 70, scale = c(3,.5),colors = brewer.pal(6,"Dark2"))
wordcloud(names(freq),freq, max.words = 25)
freq = sort(colSums(as.matrix(dtm)), decreasing = T)
wf = data.frame(word=names(freq),freq=freq)
wf = wf[1:10,]
barplot(wf$freq, names=wf$word, main="Word Frequency", xlab="Words", ylab="Count", ylim = c(0,250))
library(topicmodels)
set.seed(2019)
lda3 = LDA(dtm ,k=3, method = "Gibbs")
topics(lda3)
terms(lda3,25)
install.packages("qdap")
library(qdap)
speech16 = paste(readLines("sou2016.txt"),collapse = " ")
speech16 = iconv(speech16, "latin1","ASCII","")
prep16 = qprep(speech16)
prep16 = replace_contraction(prep16)
prep16 = rm_stopwords(prep16, Top100Words, separate = F)
prep16 = strip(prep16, char.keep = c("?","."))
sent16 = data.frame(speech = prep16)
sent16 = sentSplit(sent16, "speech")
sent16$year = "2016"
speech10 = paste(readLines("sou2010.txt"),collapse = " ")
speech10 = iconv(speech10, "latin1","ASCII","")
prep10 = qprep(speech10)
prep10 = replace_contraction(prep10)
prep10 = rm_stopwords(prep10, Top100Words, separate = F)
prep10 = strip(prep10, char.keep = c("?","."))
sent10 = data.frame(speech = prep10)
sent10 = sentSplit(sent10, "speech")
sent10$year = "2010"
sentence = data.frame(rbind(sent10,sent16))
plot(freq_terms(sentence$speech))
wordMat = wfm(sentence$speech, sentence$year)
head(wordMat[order(wordMat[,1],wordMat[,2],decreasing = T),])
trans_cloud(sentence$speech, sentence$year, min.freq = 10)
ws = word_stats(sentence$speech, sentence$year, rm.incomplete = T)
plot(ws, label = T, lab.digits = 2)
pol = polarity(sentence$speech, sentence$year)
pol
plot(pol)
pol.df = pol$all
which.min(pol.df$polarity)
pol.df$text.var[12]
ari = automated_readability_index(sentence$speech, sentence$year)
ari$Readability
div = diversity(sentence$speech, sentence$year)
div
View(ggplot)
dispersion_plot(sentence$speech, rm.vars = sentence$year, c("security","jobs","economy"), color="black", bg.color = "white")
